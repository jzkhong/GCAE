{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee627d3f-f04b-4184-b642-3426b99ed565",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0c5c17-d5f6-4be2-8867-095a0796cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import Isomap, TSNE, MDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape, UpSampling2D, Conv2DTranspose, Conv2D, MaxPool2D, Flatten, \\\n",
    "                                    Embedding, LSTM, Dense, Dropout, Input, RepeatVector, TimeDistributed, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import faiss\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Check for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(\"Num GPUs Available: \", len(physical_devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a51a40-dc79-4911-a506-125b78eb42c7",
   "metadata": {},
   "source": [
    "## Standford IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e97a76-37f9-4760-af3e-c06d7474c455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define stop words set\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags using regular expressions\n",
    "    text = re.sub(r'<.*?>', ' ', text.lower())\n",
    "    # lowercase and remove apostrophe-s at end of words\n",
    "    text = re.sub(\"'s\\\\b\", \" \", text.lower())\n",
    "    # replace single letters followed by a period with a space\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\.\", \" \", text)\n",
    "    # remove punctuation (any character that is not a word character)\n",
    "    text = re.sub(r\"[^\\w\\s]\",\" \", text)\n",
    "    # # split text into words\n",
    "    # tokens = word_tokenize(text)\n",
    "    # # perform lemmatization on words with only alphabets and remove stop words\n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() \n",
    "    #           and lemmatizer.lemmatize(token) not in stop_words]\n",
    "    # return ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Load the IMDB dataset\n",
    "ds = load_dataset(\"scikit-learn/imdb\")\n",
    "\n",
    "# Preprocess the text data for all splits\n",
    "def preprocess_function(data):\n",
    "    data['review'] = [preprocess_text(text) for text in data['review']]\n",
    "    return data\n",
    "\n",
    "ds = ds['train'].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47795cf-70a3-46df-b5c0-0c1c64ae4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into TF-IDF vectors and split data into train and test sets\n",
    "X = ds['review']\n",
    "y = np.array(list(map(lambda x: 1 if x == 'positive' else 0, ds['sentiment'])))\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134f510e-a461-445a-bf63-b37a82ef1916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9009\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42,  max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db3ee8-02e7-4141-ad8e-8482a29ff61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Convert text data to sentence embeddings\n",
    "X_train_embeddings = model.encode(X_train, convert_to_tensor=True, device=device)\n",
    "X_test_embeddings = model.encode(X_test, convert_to_tensor=True, device=device)\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "X_train_vec = X_train_embeddings.cpu().numpy()\n",
    "X_test_vec = X_test_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c562db-8828-4646-be9c-4bcf31c6e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple linear classifier (Logistic Regression)\n",
    "classifier = LogisticRegression(random_state=42,  max_iter=1000)\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict sentiment on test data\n",
    "y_pred = classifier.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f67fe-ac7e-41c4-bb4b-836957ec4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_glove_embeddings('glove.42B.300d.txt')\n",
    "embedding_dim = 300\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb80631-7a02-437f-9976-19b0812ab227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_sequence_length,\n",
    "                    trainable=False))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.005),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef71179-6fe2-4550-b9dd-39b4a3f27b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TaggedDocuments for Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(X)]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=200, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=100)\n",
    "\n",
    "# Extract document vectors\n",
    "doc_vectors = np.array([model.dv[i] for i in range(len(tagged_data))])\n",
    "\n",
    "X_train_vectors, X_test_vectors, y_train, y_test = train_test_split(doc_vectors, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8a0146-7803-4d1b-abbf-e3c4bf48d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jzkho\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_vectors, y_train)\n",
    "\n",
    "pred = model.predict(X_test_vectors)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473edff8-c22c-4426-84c6-7438567c4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = load_glove_embeddings('glove.42B.300d.txt')\n",
    "\n",
    "# Compute the average GloVe embeddings for each review\n",
    "def get_average_embeddings(text_tokens, embeddings_index, embedding_dim=300):\n",
    "    embeddings = [embeddings_index[token] for token in text_tokens if token in embeddings_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "df['embeddings'] = df['cleaned_text'].apply(lambda x: get_average_embeddings(x, embeddings_index))\n",
    "\n",
    "X = np.vstack(df['embeddings'].values)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b72ba1-1bc7-4f19-9988-c20fb71ee7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42,  max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e441307-71d8-4ab3-92ae-4dde59916699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def embed_text(text, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(text, return_tensors='tf', max_length=max_length, truncation=True, padding='max_length')\n",
    "    outputs = model(inputs)\n",
    "    return np.mean(outputs.last_hidden_state[0].numpy(), axis=0)\n",
    "\n",
    "# Embed the text data\n",
    "X_train_embeddings = []\n",
    "for text in tqdm(X_train):\n",
    "    X_train_embeddings.append(embed_text(text, tokenizer, model))\n",
    "X_train_embeddings = np.array(X_train_embeddings)\n",
    "\n",
    "X_test_embeddings = []\n",
    "for text in tqdm(X_test):\n",
    "    X_test_embeddings.append(embed_text(text, tokenizer, model))\n",
    "X_test_embeddings = np.array(X_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577a5f3-2f7e-4391-a540-04c581b9aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "\n",
    "# Define a function to process data in smaller batches\n",
    "def get_embeddings_in_batches(data, embedding_layer, batch_size=64):\n",
    "    num_samples = data.shape[0]\n",
    "    embedding_shape = (num_samples, max_sequence_length * embedding_dim)\n",
    "    embeddings_array = np.zeros(embedding_shape)\n",
    "\n",
    "    for start in tqdm(range(0, num_samples, batch_size)):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        batch_data = data[start:end]\n",
    "        batch_embeddings = embedding_layer(batch_data).numpy().reshape((batch_data.shape[0], -1))\n",
    "        embeddings_array[start:end] = batch_embeddings\n",
    "        clear_session()  # Clear the Keras session to free memory\n",
    "\n",
    "    return embeddings_array\n",
    "\n",
    "# Get the embeddings for train and test data\n",
    "X_train_embedded = get_embeddings_in_batches(X_train, embedding_layer, batch_size=64)\n",
    "X_test_embedded = get_embeddings_in_batches(X_test, embedding_layer, batch_size=64)\n",
    "\n",
    "X_train_embedded = csr_matrix(X_train_embedded)\n",
    "X_test_embedded = csr_matrix(X_test_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974d23a-f3a1-4e21-9e13-f9331cec3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)\n",
    "logistic_model.fit(X_train_embedded, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "pred = logistic_model.predict(X_test_embedded)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a5cb5-f922-4fcc-93cb-41d49867934a",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b0df7-f94c-45b6-ab07-3d413b3d2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on IMDB dataset\n",
    "pca_imdb = decomposition.PCA()\n",
    "pca_imdb.fit(X_train_vec)\n",
    "x_train_pca = pca_imdb.transform(X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a29bc2-5455-4fa8-8cbc-3771db22f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the cumulative variance explained by principal components\n",
    "cumulative_variance_ratio = np.cumsum(pca_imdb.explained_variance_ratio_)\n",
    "x_ticks = list(range(1,len(cumulative_variance_ratio)+1))\n",
    "plt.plot(x_ticks, cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5ca7a-5392-428d-96c8-b45c71f560e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise data before performing PCA\n",
    "mnist_scaler = StandardScaler()\n",
    "x_train_scaled = mnist_scaler.fit_transform(X_train_vec)\n",
    "x_test_scaled = mnist_scaler.transform(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a9ed4-9ded-41fd-8aa1-7beb97335591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on IMDB dataset\n",
    "pca_imdb = decomposition.PCA()\n",
    "pca_imdb.fit(x_train_scaled)\n",
    "x_train_pca = pca_imdb.transform(x_train_scaled)\n",
    "\n",
    "# Examine the cumulative variance explained by principal components\n",
    "cumulative_variance_ratio = np.cumsum(pca_imdb.explained_variance_ratio_)\n",
    "x_ticks = list(range(1,len(cumulative_variance_ratio)+1))\n",
    "plt.plot(x_ticks, cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84699e72-a4b2-4e11-959f-50e03f27ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on IMDB dataset\n",
    "pca_imdb = decomposition.PCA(n_components=2)\n",
    "pca_imdb.fit(x_train_scaled)\n",
    "x_train_pca = pca_imdb.transform(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cb2d4-f995-4dde-a437-accff08f22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label set and distinct colourmap\n",
    "unique_labels = set(y_train)\n",
    "\n",
    "def plot_2d_comparison(x, y, n_data, unique_labels, x_label, y_label, title):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for i in unique_labels:\n",
    "        plt.scatter(x[:n_data][y[:n_data] == i, 0], \n",
    "                    x[:n_data][y[:n_data] == i, 1], \n",
    "                    label=i, alpha=0.6)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualise the first two principal components\n",
    "plot_2d_comparison(x_train_pca, y_train, 1500, unique_labels, 'PC1', 'PC2', 'PCA - IMDB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ec357-51ad-4d08-a70d-12a0b296d22a",
   "metadata": {},
   "source": [
    "### Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e3bbf-3fe0-41ed-8df7-140ea2516044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the dataset to two-dimensional using Isomap\n",
    "isomap_imdb = Isomap(n_components=2, n_neighbors=50) \n",
    "# Fit the isomap algorithn using the first 30,000 data points to reduce compute time\n",
    "x_train_isomap = isomap_imdb.fit_transform(X_train_vec[:10000]) \n",
    "\n",
    "# Visualise the 2d representation of the dataset\n",
    "plot_2d_comparison(x_train_isomap, y_train, 1500, unique_labels, \n",
    "               'Dim1', 'Dim2', 'Isomap - IMDB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010bbc3-3ea6-418e-a4b0-02c7e680d605",
   "metadata": {},
   "source": [
    "### Geodesic distance + MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a6431-a307-487e-8467-754eaa236303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT-compiled function for constructing the graph\n",
    "@jit(nopython=True)\n",
    "def construct_graph(indices, distances, n_neighbors, n_samples):\n",
    "    graph = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(1, n_neighbors):  # start from 1 to skip self-loop\n",
    "            graph[i, indices[i, j]] = distances[i, j]\n",
    "            graph[indices[i, j], i] = distances[i, j]\n",
    "    return graph\n",
    "\n",
    "# JIT-compiled function for constructing the expanded graph\n",
    "@jit(nopython=True)\n",
    "def construct_expanded_graph(indices, distances, neigh_graph, n_neighbors, n_train, n_samples):\n",
    "    n_total = n_train + n_samples\n",
    "    graph = np.zeros((n_total, n_total))\n",
    "    graph[:n_train, :n_train] = neigh_graph\n",
    "    for i in range(n_samples):\n",
    "        for j in range(1, n_neighbors):  # start from 1 to skip self-loop\n",
    "            graph[n_train+i, indices[i, j]] = distances[i, j]\n",
    "            graph[indices[i, j], n_train+i] = distances[i, j]\n",
    "    return graph\n",
    "\n",
    "def compute_geodesic_distances(data, n_neighbors=10, training=False, faiss_index=None, neigh_graph=None):\n",
    "    print(f\"Memory usage at start: {memory_usage_psutil()} MB\")\n",
    "\n",
    "    if training:\n",
    "        # Use FAISS to find approximate nearest neighbors\n",
    "        d = data.shape[1]\n",
    "        faiss_index = faiss.IndexFlatL2(d)\n",
    "        faiss_index.add(data)\n",
    "        distances, indices = faiss_index.search(data, n_neighbors)\n",
    "\n",
    "        # Construct sparse neighborhood graph\n",
    "        n_samples = data.shape[0]\n",
    "        print(f\"Memory usage before constructing graph: {memory_usage_psutil()} MB\")\n",
    "        graph = construct_graph(indices, distances, n_neighbors, n_samples)\n",
    "        print(f\"Memory usage after constructing graph: {memory_usage_psutil()} MB\")\n",
    "\n",
    "        # Compute geodesic distances using the shortest path algorithm\n",
    "        print(f\"Memory usage before shortest path: {memory_usage_psutil()} MB\")\n",
    "        geodesic_distances = shortest_path(csgraph=graph, method='D', directed=False)\n",
    "        print(f\"Memory usage after shortest path: {memory_usage_psutil()} MB\")\n",
    "    else:\n",
    "        # Use trained FAISS index to find approximate nearest neighbors\n",
    "        n_train = faiss_index.ntotal\n",
    "        n_samples = data.shape[0]\n",
    "        print(f\"Memory usage before searching: {memory_usage_psutil()} MB\")\n",
    "        distances, indices = faiss_index.search(data, n_neighbors)\n",
    "\n",
    "        # Construct sparse neighborhood graph\n",
    "        print(f\"Memory usage before constructing expanded graph: {memory_usage_psutil()} MB\")\n",
    "        graph = construct_expanded_graph(indices, distances, neigh_graph, n_neighbors, n_train, n_samples)\n",
    "        print(f\"Memory usage after constructing expanded graph: {memory_usage_psutil()} MB\")\n",
    "\n",
    "        # Compute geodesic distances using the shortest path algorithm for all data\n",
    "        print(f\"Memory usage before shortest path: {memory_usage_psutil()} MB\")\n",
    "        geodesic_distances = shortest_path(csgraph=graph, method='D', directed=False)\n",
    "        print(f\"Memory usage after shortest path: {memory_usage_psutil()} MB\")\n",
    "\n",
    "        # Extract the geodesic distances matrix for only new data\n",
    "        geodesic_distances = geodesic_distances[n_train:, n_train:]\n",
    "    \n",
    "    # Explicitly call garbage collector\n",
    "    gc.collect()\n",
    "    print(f\"Memory usage after garbage collection: {memory_usage_psutil()} MB\")\n",
    "\n",
    "    return geodesic_distances, faiss_index, graph\n",
    "\n",
    "start_time = time.time()\n",
    "# Compute geodesic distances for mnist dataset (training)\n",
    "n_neighbors = 5\n",
    "geodesic_distances, faiss_index, neigh_graph = compute_geodesic_distances(X_train, n_neighbors=n_neighbors, training=True)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Time taken to run the function: {elapsed_time} seconds\")\n",
    "# 15000: 117s\n",
    "\n",
    "print(geodesic_distances.shape)\n",
    "print(faiss_index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fb855-28c7-4831-ae9a-b3f846efe308",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "n_neighbors = 10\n",
    "data = X_train\n",
    "\n",
    "# Use FAISS to find approximate nearest neighbors\n",
    "d = data.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_index.add(data)\n",
    "distances, indices = faiss_index.search(data, n_neighbors)\n",
    "\n",
    "# Construct sparse neighborhood graph\n",
    "n_samples = data.shape[0]\n",
    "graph = np.zeros((n_samples, n_samples))\n",
    "for i in range(n_samples):\n",
    "    for j in range(1, n_neighbors):  # start from 1 to skip self-loop\n",
    "        graph[i, indices[i, j]] = distances[i, j]\n",
    "        graph[indices[i, j], i] = distances[i, j]\n",
    "\n",
    "graph = csr_matrix(graph)\n",
    "\n",
    "# Compute geodesic distances using the shortest path algorithm\n",
    "geodesic_distances = shortest_path(csgraph=graph, method='D', directed=False)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Time taken to run the function: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a056724-24c2-4a67-9347-64d618886250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the numpy array to a CSV file\n",
    "np.savetxt(\"geo_dist_imdb.csv\", geodesic_distances, delimiter=\",\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8178d2-7e63-4ea9-8dab-fcf7c03798ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "geodesic_distances = np.loadtxt(\"geo_dist_imdb.csv\", delimiter=\",\", dtype=np.float32)\n",
    "\n",
    "# Verify the loaded data\n",
    "print(\"Loaded data shape:\", geodesic_distances.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eccbc04-7c5e-475a-ab72-3dd0565ff34f",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854d8d0-17fd-42c1-bac5-e6025cd1fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the dataset to two-dimensional using t-SNE\n",
    "tsne_imdb = TSNE(n_components=2, init='random') \n",
    "x_train_tsne = tsne_imdb.fit_transform(X_train_vec[:10000])\n",
    "\n",
    "# Visualise the 2d representation of the dataset\n",
    "plot_2d_comparison(x_train_tsne, y_train, 1500, unique_labels,\n",
    "               'Dim1', 'Dim2', 't-SNE - MNIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451237f-8cc4-4275-8f8d-4501bfcf3bca",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16b7d4-9901-4835-903b-eeea8b9db138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "maxlen = 1000\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Prepare labels\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2346957-ad89-4d53-a044-e1166e849fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_file = 'glove.6B.50d.txt'  # Adjust path and file according to your download\n",
    "embeddings_index = load_glove_embeddings(glove_file)\n",
    "\n",
    "def create_embedding_matrix(word_index, embeddings_index, embedding_dim):\n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(word_index, embeddings_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57975e0-f797-4de6-8776-f184a0f6085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embeddings = Embedding(input_dim=len(word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dd898-34d4-45a4-9323-1e7aceb3312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the encodings after training\n",
    "untrained_encodings_mnist = mnist_encoder(x_test).numpy()\n",
    "\n",
    "# Visualise the trained latent encodings\n",
    "plot_2d_mnist(untrained_encodings_mnist, y_test, 3000, unique_labels, \n",
    "               'Dim1', 'Dim2', 'Autoencoder - MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e14af-02fa-49d0-ba3f-3cbc1e881aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects for train and test sets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, x_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, x_test))\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "shuffle_buffer_size = 1000\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer_size, seed).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create an EarlyStopping callback to terminate training if the validation total loss doesn't immprove after 10 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Compile and fit the model\n",
    "mnist_autoencoder.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "history_mnist = mnist_autoencoder.fit(train_dataset, validation_data=test_dataset, epochs=100, callbacks=early_stopping, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105789c-470e-45a2-bc41-552d19c2236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final training and validation loss (MSE)\n",
    "final_train_loss = history_mnist.history['loss'][-1]\n",
    "final_val_loss = history_mnist.history['val_loss'][-1]\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}, Final Validation Loss: {final_val_loss:.4f}\")\n",
    "\n",
    "# Examine the training and validation loss (MSE) over epochs\n",
    "plt.plot(history_mnist.history['loss'], label='train')\n",
    "plt.plot(history_mnist.history['val_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff97360-7286-4dd8-a2c2-ef1b628fe9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the encodings after training\n",
    "trained_encodings_mnist = mnist_encoder(x_test).numpy()\n",
    "\n",
    "# Visualise the trained latent encodings\n",
    "plot_2d_mnist(trained_encodings_mnist, y_test, 1000, unique_labels, \n",
    "               'Dim1', 'Dim2', 'Autoencoder - MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe767b-155f-4bb1-864e-ada25952479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "inx = np.random.choice(x_test.shape[0], 10, replace=False)\n",
    "reconstructed_mnist_images = mnist_autoencoder(x_test[inx])\n",
    "\n",
    "f, axs = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for j in range(10):\n",
    "    axs[0, j].imshow(x_test[inx][j], cmap='binary')\n",
    "    axs[1, j].imshow(reconstructed_mnist_images[j].numpy().squeeze(), cmap='binary')\n",
    "    axs[0, j].axis('off')\n",
    "    axs[1, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6c55a-eccd-4fe5-91e7-3a1bab451f14",
   "metadata": {},
   "source": [
    "### GCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d511b-cc31-4dbc-b6f1-1ad75d2518c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCAETrainer(Model):\n",
    "    \"\"\"\n",
    "    A trainer class for a GCAE that handles model training and evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, input_dim=(None,None,1), embedding_dim=2, n_neighbors=10, alpha=0.01, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dataset = dataset\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.gcae = get_cnn_autoencoder(self.input_dim, self.embedding_dim, self.dataset)\n",
    "        self.encoder = self.gcae.get_layer(f'{self.dataset}_cnn_encoder')\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # FAISS index and training data\n",
    "        self.faiss_index = None\n",
    "        self.neigh_graph = None\n",
    "        self.train_geo_dist = None\n",
    "        self.train_geo_dist_max = None\n",
    "        self.test_geo_dist = None\n",
    "\n",
    "        # Define the loss metrics to track and log\n",
    "        self.total_loss_metric = Mean(name='total_loss')\n",
    "        self.reconstruction_loss_metric = Mean(name='reconstruction_loss')\n",
    "        self.geodesic_loss_metric = Mean(name='geodesic_loss')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Returns a list of metrics used in training and evaluation.\n",
    "        \"\"\"\n",
    "        return [self.total_loss_metric, self.reconstruction_loss_metric, self.geodesic_loss_metric]\n",
    "\n",
    "    def compute_geodesic_distances(self, data, training=False):\n",
    "    \n",
    "        if training:\n",
    "            \n",
    "            # Use FAISS to find approximate nearest neighbors\n",
    "            d = data.shape[1]\n",
    "            faiss_index = faiss.IndexFlatL2(d)\n",
    "            faiss_index.add(data)\n",
    "            self.faiss_index = faiss_index\n",
    "            distances, indices = faiss_index.search(data, self.n_neighbors)\n",
    "        \n",
    "            # Construct sparse neighborhood graph\n",
    "            n_samples = data.shape[0]\n",
    "            graph = np.zeros((n_samples, n_samples))\n",
    "            for i in range(n_samples):\n",
    "                for j in range(1, self.n_neighbors):  # start from 1 to skip self-loop\n",
    "                    graph[i, indices[i, j]] = np.sqrt(distances[i, j])\n",
    "                    graph[indices[i, j], i] = np.sqrt(distances[i, j])          \n",
    "            graph = csr_matrix(graph)\n",
    "            self.neigh_graph = graph\n",
    "        \n",
    "            # Compute geodesic distances using the shortest path algorithm\n",
    "            geodesic_distances = shortest_path(csgraph=graph, method='D', directed=False)\n",
    "            geodesic_distances = np.float32(geodesic_distances)\n",
    "            self.train_geo_dist = geodesic_distances\n",
    "            self.train_geo_dist_max = np.max(geodesic_distances)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            # Use trained FAISS index to find approximate nearest neighbors\n",
    "            n_train = self.faiss_index.ntotal\n",
    "            n_samples = tf.shape(data)[0]\n",
    "            n_total = n_train + n_samples\n",
    "            distances, indices = self.faiss_index.search(data, self.n_neighbors)\n",
    "    \n",
    "            # Construct sparse neighborhood graph\n",
    "            graph = np.zeros((n_total, n_total))\n",
    "            graph[:n_train, :n_train] = self.neigh_graph\n",
    "            for i in range(n_samples):\n",
    "                for j in range(1, self.n_neighbors):  # start from 1 to skip self-loop\n",
    "                    graph[n_train+i, indices[i, j]] = np.sqrt(distances[i, j])\n",
    "                    graph[indices[i, j], n_train+i] = np.sqrt(distances[i, j])\n",
    "            graph = csr_matrix(graph)\n",
    "            \n",
    "            # Compute geodesic distances using the shortest path algorithm for all data\n",
    "            geodesic_distances = shortest_path(csgraph=graph, method='D', directed=False)\n",
    "    \n",
    "            # Extract the geodesic distances matrix for only new data\n",
    "            geodesic_distances = geodesic_distances[n_train:, n_train:]\n",
    "            geodesic_distances = np.float32(geodesic_distances)\n",
    "            self.test_geo_dist = geodesic_distances\n",
    "            \n",
    "        return geodesic_distances\n",
    "    \n",
    "    def compute_geodesic_loss(self, z, geodesic_distances, batch_indices):\n",
    "        \"\"\"\n",
    "        Computes geodesic distance loss.\n",
    "        \"\"\"\n",
    "        # Calculate the pairwise Euclidean distances in the latent space\n",
    "        z_distances = tf.norm(z[:, tf.newaxis] - z, axis=2)\n",
    "        # Calculate the geodesic distance loss\n",
    "        batch_indices_np = batch_indices.numpy()\n",
    "        geodesic_distances_batch = geodesic_distances[np.ix_(batch_indices_np, batch_indices_np)]\n",
    "        # geodesic_distances_batch = tf.gather(tf.gather(geodesic_distances, batch_indices, axis=0), batch_indices, axis=1)\n",
    "        distance_loss = tf.reduce_mean((z_distances - geodesic_distances_batch) ** 2) / self.train_geo_dist_max\n",
    "        return distance_loss\n",
    "        \n",
    "    def _get_losses(self, x, batch_indices, training=False):\n",
    "        \"\"\"\n",
    "        Computes model losses from inputs.\n",
    "        \"\"\"\n",
    "        reconstructions = self.gcae(x, training=training)\n",
    "        latent = self.encoder(x)\n",
    "        # Compute the reconstruction loss\n",
    "        x = tf.expand_dims(x, axis=-1)\n",
    "        reconstruction_loss = (\n",
    "            tf.reduce_mean((x - reconstructions) ** 2)\n",
    "        )\n",
    "        # Compute the geodesic loss\n",
    "        if training:\n",
    "            geodesic_loss = self.compute_geodesic_loss(z=latent, geodesic_distances=self.train_geo_dist, \n",
    "                                                       batch_indices=batch_indices)\n",
    "        else:    \n",
    "            geodesic_loss = self.compute_geodesic_loss(z=latent, geodesic_distances=self.test_geo_dist, \n",
    "                                                       batch_indices=batch_indices)\n",
    "        # Cmpute the total loss\n",
    "        total_loss = reconstruction_loss + self.alpha * geodesic_loss\n",
    "        return total_loss, reconstruction_loss, geodesic_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Performs one training step using a single batch of data.\n",
    "        \"\"\"\n",
    "        x, batch_indices = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, reconstruction_loss, geodesic_loss = self._get_losses(x, batch_indices, training=True)\n",
    "        grads = tape.gradient(total_loss, self.gcae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.gcae.trainable_variables))\n",
    "\n",
    "        # Update loss metrics\n",
    "        self.total_loss_metric.update_state(total_loss)\n",
    "        self.reconstruction_loss_metric.update_state(reconstruction_loss)\n",
    "        self.geodesic_loss_metric.update_state(geodesic_loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \"\"\"\n",
    "        Evaluates the model using a single batch of data.\n",
    "        \"\"\"\n",
    "        x, batch_indices = data\n",
    "        total_loss, reconstruction_loss, geodesic_loss = self._get_losses(x, batch_indices, training=False)\n",
    "\n",
    "        # Update loss metrics\n",
    "        self.total_loss_metric.update_state(total_loss)\n",
    "        self.reconstruction_loss_metric.update_state(reconstruction_loss)\n",
    "        self.geodesic_loss_metric.update_state(geodesic_loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "# Define the gcae trainer\n",
    "gcae_trainer = GCAETrainer(input_dim=input_shape, embedding_dim=encoded_dim, n_neighbors=10, dataset='mnist')\n",
    "\n",
    "# Ensure eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# # Generate Geodesic distance matrix for train and validation data\n",
    "# train_start_time = time.time()\n",
    "# train_geo_dist = gcae_trainer.compute_geodesic_distances(X_train, training=True)\n",
    "# train_end_time = time.time()\n",
    "# print(f\"Time taken to compute geodesic distances for train data: {train_end_time - train_start_time} seconds\") # 2393.357335090637 seconds\n",
    "\n",
    "# # Save the numpy array to a CSV file\n",
    "# np.savetxt(\"mnist_train_geo_dist.csv\", gcae_trainer.train_geo_dist, delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "# val_start_time = time.time()\n",
    "# val_geo_dist = gcae_trainer.compute_geodesic_distances(X_test, training=False)\n",
    "# val_end_time = time.time()\n",
    "# print(f\"Time taken to compute geodesic distances for val data: {val_end_time - val_start_time} seconds\") # 3650.7034978866577 seconds\n",
    "\n",
    "# # Save the numpy array to a CSV file\n",
    "# np.savetxt(\"mnist_test_geo_dist.csv\", gcae_trainer.test_geo_dist, delimiter=\",\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8325b-51cb-42d8-a295-34acefbde333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "train_geo_dist = np.loadtxt(\"mnist_train_geo_dist.csv\", delimiter=\",\", dtype=np.float32)\n",
    "val_geo_dist = np.loadtxt(\"mnist_test_geo_dist.csv\", delimiter=\",\", dtype=np.float32)\n",
    "\n",
    "# Verify the loaded data\n",
    "print(\"Loaded MNIST train geo dist shape:\", train_geo_dist.shape)\n",
    "print(\"Loaded MNIST test geo dist shape:\", val_geo_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d141c-8d0c-407d-8111-7dd23f56ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcae_trainer.train_geo_dist = train_geo_dist\n",
    "gcae_trainer.train_geo_dist_max = np.max(train_geo_dist)\n",
    "gcae_trainer.test_geo_dist = val_geo_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba3599-a81a-4910-ac28-847db1515d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to TensorFlow Dataset\n",
    "batch_size = 64\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), np.arange(len(x_train))))\n",
    "train_data = train_data.batch(batch_size)\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_test.astype(np.float32), np.arange(len(x_test))))\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "# Create an EarlyStopping callback to terminate training if the validation total loss doesn't immprove after 10 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_total_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Train the autoencoder over 100 epochs\n",
    "gcae_trainer.compile(optimizer='adam')\n",
    "history_gc_mnist = gcae_trainer.fit(train_data, validation_data=val_data, epochs=100, callbacks=early_stopping, verbose=0)\n",
    "\n",
    "# Print final training and validation loss (MSE)\n",
    "final_train_loss = history_gc_mnist.history['total_loss'][-1]\n",
    "final_val_loss = history_gc_mnist.history['val_total_loss'][-1]\n",
    "print(f\"Final Training Loss: {final_train_loss:.4f}, Final Validation Loss: {final_val_loss:.4f}\")\n",
    "\n",
    "# Examine the training and validation loss (MSE) over epochs\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(history_gc_mnist.history['total_loss'], label='train')\n",
    "plt.plot(history_gc_mnist.history['val_total_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Total Loss (MSE + Geodesic Loss)\")\n",
    "plt.title(\"Total Loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(history_gc_mnist.history['reconstruction_loss'], label='train')\n",
    "plt.plot(history_gc_mnist.history['val_reconstruction_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Reconstruction Loss vs epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(history_gc_mnist.history['geodesic_loss'], label='train')\n",
    "plt.plot(history_gc_mnist.history['val_geodesic_loss'], label='val')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Geodesic Loss\")\n",
    "plt.title(\"Geodesic Loss vs epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad19007-1259-4c9e-952e-e277e3117046",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_gc_encoder = gcae_trainer.gcae.get_layer('mnist_cnn_encoder')\n",
    "\n",
    "# Compute the encodings after training\n",
    "trained_encodings_gc_mnist = mnist_gc_encoder(x_test).numpy()\n",
    "\n",
    "# Visualise the trained latent encodings\n",
    "plot_2d_mnist(trained_encodings_gc_mnist, y_test, 1000, unique_labels, \n",
    "               'Dim1', 'Dim2', 'GCAE - MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd3551-8762-424d-9c14-b4a4e30e2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_gc_autoencoder = gcae_trainer.gcae\n",
    "\n",
    "np.random.seed(5)\n",
    "inx = np.random.choice(x_test.shape[0], 10, replace=False)\n",
    "reconstructed_mnist_images_gc = mnist_gc_autoencoder(x_test[inx])\n",
    "\n",
    "f, axs = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for j in range(10):\n",
    "    axs[0, j].imshow(x_test[inx][j], cmap='binary')\n",
    "    axs[1, j].imshow(reconstructed_mnist_images_gc[j].numpy().squeeze(), cmap='binary')\n",
    "    axs[0, j].axis('off')\n",
    "    axs[1, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572e953-8456-49f9-9c2b-c91c6fb28985",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f507f-4825-4373-b59b-cf5860c4c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic classifier using original data\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate accuracy using test data\n",
    "pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc362fe1-9450-46ce-85cf-aef323c99287",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "input_shape = (28, 28, 1)\n",
    "dims = [2, 10, 20, 50, 100]\n",
    "accs = []\n",
    "\n",
    "for d in dims:\n",
    "    # Build and train the CNN autoencoder for the MNIST dataset\n",
    "    mnist_autoencoder = get_cnn_autoencoder(input_shape, d, 'mnist')\n",
    "    mnist_encoder = mnist_autoencoder.get_layer('mnist_cnn_encoder')\n",
    "    mnist_autoencoder.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "    history_mnist = mnist_autoencoder.fit(train_dataset, validation_data=test_dataset, epochs=100, callbacks=early_stopping, verbose=0)\n",
    "\n",
    "    # Generate encodings for train data\n",
    "    trained_mnist_encodings = mnist_encoder(x_train).numpy()\n",
    "\n",
    "    # Train logistic classifier using standard autoencoder's embeddings\n",
    "    model = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    model.fit(trained_mnist_encodings, y_train)\n",
    "\n",
    "    # Generate encodings for test data\n",
    "    predicted_mnist_encodings = mnist_encoder(x_test).numpy()\n",
    "    \n",
    "    # Validate classification accuracy on test data\n",
    "    pred = model.predict(predicted_mnist_encodings)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    accs.append(acc)\n",
    "    print(f'Accuracy with {d} dimensions: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de0627-4af7-421a-89f1-afa01aafad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "input_shape = (28, 28, 1)\n",
    "dims = [2, 10, 20, 50, 100]\n",
    "accs = []\n",
    "\n",
    "for d in dims:\n",
    "    # Build and train the CNN autoencoder for the MNIST dataset\n",
    "    mnist_autoencoder = get_cnn_autoencoder(input_shape, d, 'mnist')\n",
    "    mnist_encoder = mnist_autoencoder.get_layer('mnist_cnn_encoder')\n",
    "    mnist_autoencoder.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "    history_mnist = mnist_autoencoder.fit(train_dataset, validation_data=test_dataset, epochs=100, callbacks=early_stopping, verbose=0)\n",
    "\n",
    "    # Generate encodings for train data\n",
    "    trained_mnist_encodings = mnist_encoder(x_train).numpy()\n",
    "\n",
    "    # Train logistic classifier using standard autoencoder's embeddings\n",
    "    model = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    model.fit(trained_mnist_encodings, y_train)\n",
    "\n",
    "    # Generate encodings for test data\n",
    "    predicted_mnist_encodings = mnist_encoder(x_test).numpy()\n",
    "    \n",
    "    # Validate classification accuracy on test data\n",
    "    pred = model.predict(predicted_mnist_encodings)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    accs.append(acc)\n",
    "    print(f'Accuracy with {d} dimensions: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
